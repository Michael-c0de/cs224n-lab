{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a7cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8db5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code. \n",
    "\n",
    "    Arguments:\n",
    "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        tmp = np.max(x, axis=1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis=1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        # Vector\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a323e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "outsideWordVecs=np.random.rand(100,10)\n",
    "centerWordVecs=np.random.rand(100,10)\n",
    "\n",
    "centerWordVector=centerWordVecs[1]\n",
    "\n",
    "softmax(np.dot(outsideWordVecs,centerWordVector)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "c006bef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "5472e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    outsideVectors[10]=x\n",
    "    return -np.log(softmax(np.dot(outsideVectors,centerWordVec)))[outsideWordIdx],np.array(((y_c*y4)/y3)).repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "d805280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check failed for gradientText.\n",
      "First gradient error found at index (0,) in the vector of gradients\n",
      "Your gradient: 0.006209 \t Numerical gradient: 0.004677\n"
     ]
    }
   ],
   "source": [
    "gradcheck_naive(func, outsideVectors[10], 'gradientText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "a10f83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outsideVectors=np.random.rand(100,10)\n",
    "centerWordVec=np.random.rand(10)\n",
    "outsideWordIdx=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "77cb749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    y_=softmax(np.dot(outsideVectors,x))\n",
    "    y=np.zeros(outsideVectors.shape[0])\n",
    "    y[outsideWordIdx]=1\n",
    "    \n",
    "    return -np.log(y_[outsideWordIdx]),np.dot((y_[outsideWordIdx]-1),outsideVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "7a4e7207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9924198937504909"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_[outsideWordIdx]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "6ef6afec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1416\\4216158493.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgradcheck_naive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcenterWordVec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'gradCenterVec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1416\\1405304686.py\u001b[0m in \u001b[0;36mgradcheck_naive\u001b[1;34m(f, x, gradientText)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Compare gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mreldiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumgrad\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreldiff\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Gradient check failed for %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgradientText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "gradcheck_naive(func,centerWordVec,'gradCenterVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "407c9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(x):\n",
    "    y_=softmax(np.dot(x,centerWordVec))\n",
    "    sub=np.zeros_like(y_)\n",
    "    sub[outsideWordIdx]=1\n",
    "    y_sub=y_-sub\n",
    "    gradOutsideVecs=np.array([centerWordVec * y_sub[i] for i in range(len(y_sub))])\n",
    "    \n",
    "    return -np.log(y_[outsideWordIdx]),gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "630fb538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "gradcheck_naive(func2,outsideVectors,'gradOutsideVecs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "370e921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE (~1 Line)\n",
    "    s=1/(1+np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "bca3a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10,)\n",
      "(11, 10)\n"
     ]
    }
   ],
   "source": [
    "print(centerWordVec.shape) #中心词\n",
    "print(outsideVectors[outsideWordIdx].shape) #预测正样本\n",
    "negSampleWordIndices=[i for i in range(10,21)]\n",
    "print(outsideVectors[negSampleWordIndices].shape) #预测正样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "e61814b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7120182581552803"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "aeaf2c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7634282773443037"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#目标函数\n",
    "r=-np.log(sigmoid(np.dot(outsideVectors[outsideWordIdx],centerWordVec)))\n",
    "l=-np.log(sigmoid(np.dot(-np.concatenate([outsideVectors[negSampleWordIndices]]),centerWordVec))).sum()\n",
    "l+r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "1a72a9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.06602457, 2.02811054, 0.89595356, 1.738278  , 0.29081038,\n",
       "       1.1979591 , 2.15037812, 0.90683858, 3.01771636, 0.3440505 ])"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对V_c之偏导\n",
    "left=((1-sigmoid(np.dot(-np.concatenate([outsideVectors[negSampleWordIndices]]),centerWordVec))) @ outsideVectors[negSampleWordIndices]).sum()\n",
    "right=-centerWordVec*(1-sigmoid(np.dot(outsideVectors[outsideWordIdx],centerWordVec)))\n",
    "left+right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "d27c8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_v_c(x):\n",
    "    centerWordVec=x\n",
    "    tmpl=sigmoid(np.dot(outsideVectors[outsideWordIdx],centerWordVec))\n",
    "    tmpr=sigmoid(np.dot(-np.concatenate([outsideVectors[negSampleWordIndices]]),centerWordVec))\n",
    "    \n",
    "    obj=-np.log(tmpl)-np.log(tmpr).sum()\n",
    "    \n",
    "    #对V_c之偏导\n",
    "    grad=((1-tmpr) @\\\n",
    "          np.concatenate([outsideVectors[negSampleWordIndices]]))-\\\n",
    "    outsideVectors[outsideWordIdx]*(1-tmpl)\n",
    "    return obj,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "2eefaa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "gradcheck_naive(grad_v_c,centerWordVec,'grad_v_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "80daaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_v_c(x):\n",
    "    outsideVectors[outsideWordIdx]=x\n",
    "    tmpl=sigmoid(np.dot(outsideVectors[outsideWordIdx],centerWordVec))\n",
    "    tmpr=sigmoid(np.dot(-np.concatenate([outsideVectors[negSampleWordIndices]]),centerWordVec))\n",
    "    \n",
    "    obj=-np.log(tmpl)-np.log(tmpr).sum()\n",
    "    \n",
    "    #对U0之偏导\n",
    "    grad=-centerWordVec * (1-tmpl)\n",
    "    return obj,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "6fe72410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "gradcheck_naive(grad_v_c,outsideVectors[outsideWordIdx],'grad_v_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "e9c197cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_v_c(x):\n",
    "    \n",
    "    tmpl=sigmoid(np.dot(outsideVectors[outsideWordIdx],centerWordVec))\n",
    "    tmpr=sigmoid(np.dot(-x,centerWordVec))\n",
    "    \n",
    "    obj=-np.log(tmpl)-np.log(tmpr).sum()\n",
    "    \n",
    "    #对Uw之偏导\n",
    "    grad=np.array([centerWordVec * (1-i) for i in tmpr])\n",
    "    return obj,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "bbe1fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "x=np.concatenate([outsideVectors[negSampleWordIndices]])\n",
    "gradcheck_naive(grad_v_c,x,'grad_v_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "34a47a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    grad1=-centerWordVec * (1-tmpl)\n",
    "    grad2=np.array([centerWordVec * (1-i) for i in tmpr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "4178822e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "4decc53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 10)"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "46e186a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 10)"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([grad1.reshape(1,-1),grad2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "6af67671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcheck_naive(f, x, gradientText):\n",
    "    \"\"\" Gradient check for a function f.\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         loss and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    gradientText -- a string detailing some context about the gradient computation\n",
    "\n",
    "    Notes:\n",
    "    Note that gradient checking is a sanity test that only checks whether the\n",
    "    gradient and loss values produced by your implementation are consistent with\n",
    "    each other. Gradient check passing on its own doesn’t guarantee that you\n",
    "    have the correct gradients. It will pass, for example, if both the loss and\n",
    "    gradient values produced by your implementation are 0s (as is the case when\n",
    "    you have not implemented anything). Here is a detailed explanation of what\n",
    "    gradient check is doing if you would like some further clarification:\n",
    "    http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/. \n",
    "    \"\"\"\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4        # Do not change this!\n",
    "\n",
    "    # Iterate over all indexes ix in x to check the gradient.\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        x[ix] += h # increment by h\n",
    "        random.setstate(rndstate)\n",
    "        fxh, _ = f(x) # evalute f(x + h)\n",
    "        x[ix] -= 2 * h # restore to previous value (very important!)\n",
    "        random.setstate(rndstate)\n",
    "        fxnh, _ = f(x)\n",
    "        x[ix] += h\n",
    "        numgrad = (fxh - fxnh) / 2 / h\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-6:\n",
    "            print(\"Gradient check failed for %s.\" % gradientText)\n",
    "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad))\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print(\"Gradient check passed!. Read the docstring of the `gradcheck_naive`\"\n",
    "    \" method in utils.gradcheck.py to understand what the gradient check does.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "19d3620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N,1)) + 1e-30\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "eb0444e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDummyObjects():\n",
    "    \"\"\" Helper method for naiveSoftmaxLossAndGradient and negSamplingLossAndGradient tests \"\"\"\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "\n",
    "    dataset = type('dummy', (), {})()\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "    return dataset, dummy_vectors, dummy_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "e60132d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    ### YOUR CODE HERE (~6-8 Lines)\n",
    "    y_=softmax(np.dot(outsideVectors,centerWordVec))\n",
    "    loss=-np.log(y_)[outsideWordIdx]\n",
    "    y=np.zeros(outsideVectors.shape[0])\n",
    "    y[outsideWordIdx]=1\n",
    "    gradCenterVec=np.dot((y_-y),outsideVectors)\n",
    "    \n",
    "    sub=np.zeros_like(y_)\n",
    "    sub[outsideWordIdx]=1\n",
    "    y_sub=y_-sub\n",
    "    gradOutsideVecs=np.array([centerWordVec * y_sub[i] for i in range(len(y_sub))])\n",
    "    \n",
    "    \n",
    "    ### Please use the provided softmax function (imported earlier in this file)\n",
    "    ### This numerically stable implementation helps you avoid issues pertaining\n",
    "    ### to integer overflow. \n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "8b95dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_naiveSoftmaxLossAndGradient():\n",
    "    \"\"\" Test naiveSoftmaxLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for naiveSoftmaxLossAndGradient ====\")\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(vec, 1, dummy_vectors, dataset)\n",
    "        return loss, gradCenterVec\n",
    "    gradcheck_naive(temp, np.random.randn(3), \"naiveSoftmaxLossAndGradient gradCenterVec\")\n",
    "\n",
    "    centerVec = np.random.randn(3)\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "    gradcheck_naive(temp, dummy_vectors, \"naiveSoftmaxLossAndGradient gradOutsideVecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "0b6ae540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "test_naiveSoftmaxLossAndGradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "id": "b49abf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss and gradients for a centerWordVec\n",
    "    and a outsideWordIdx word vector as a building block for word2vec\n",
    "    models. K is the number of negative samples to take.\n",
    "\n",
    "    Note: The same word may be negatively sampled multiple times. For\n",
    "    example if an outside word is sampled twice, you shall have to\n",
    "    double count the gradient with respect to this word. Thrice if\n",
    "    it was sampled three times, and so forth.\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Negative sampling of words is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    ### YOUR CODE HERE (~10 Lines)\n",
    "    ### Please use your implementation of sigmoid in here.\n",
    "    tmpl=sigmoid(np.dot(outsideVectors[outsideWordIdx],centerWordVec))\n",
    "    tmpr=sigmoid(np.dot(-np.concatenate([outsideVectors[negSampleWordIndices]]),centerWordVec))\n",
    "    loss=-np.log(tmpl)-np.log(tmpr).sum()\n",
    "    \n",
    "    gradCenterVec=((1-tmpr) @ np.concatenate([outsideVectors[negSampleWordIndices]]))-outsideVectors[outsideWordIdx]*(1-tmpl)\n",
    "    \n",
    "    gradu0=-centerWordVec * (1-tmpl)\n",
    "    graduw=np.array([centerWordVec * (1-i) for i in tmpr])\n",
    "    grad=np.concatenate([gradu0.reshape(1,-1),graduw])\n",
    "    gradOutsideVecs=np.zeros_like(outsideVectors)\n",
    "    \n",
    "    \n",
    "    for k,v in enumerate(indices):\n",
    "        gradOutsideVecs[v]+=grad[k]\n",
    "    ### END YOUR CODE\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "8517c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "cf56204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "centerVec = np.random.randn(3)\n",
    "def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = negSamplingLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "gradcheck_naive(temp, dummy_vectors, \"negSamplingLossAndGradient gradOutsideVecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "31360e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "96894c6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dummy' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1416\\1943890687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dummy' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "id": "4b445208",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "id": "9ab4765a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1074925126.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Michael\\AppData\\Local\\Temp\\ipykernel_1416\\1074925126.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    i++\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "i++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711bf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'cs224n'",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
